{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Which of these accurately describes the relationship between Apache Beam and Cloud Dataflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: Apache Beam is the API for data pipeline building in java or python and Cloud Dataflow is the implementation and execution framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. TRUE or FALSE: The Filter method can be carried out in parallel and autoscaled by the execution framework:\n",
    "<img src=\"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/A8aupUykEeiqUw7FBf_ScA_4b3a6c68b31f46fa68096ace9520dcbe_graph.jpg?expiry=1544054400000&amp;hmac=CIOYUJP5MegcEw1Z-480l3VegAKzRvpP2ELFgmtVOJ8\" alt=\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the purpose of a Cloud Dataflow connector?\n",
    "\n",
    ".apply(TextIO.write().to(“gs://…”));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: Connectors allow you to output the results of a pipeline to a specific data sink like Bigtable, Google Cloud Storage, flat file, BigQuery, and more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Below you'll find a Cloud Dataflow preprocessing graph. Correctly identify the terms for A, B, and C.\n",
    "<img src=\"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/DNcgbkylEeiErArshWmU7g_ee7386e8034b923d76176e799b3e0b48_dag.jpg?expiry=1544054400000&amp;hmac=A5j7ptWHwefrR7HnKGsggn2ocfY-j44Ks80nkHfCreU\" alt=\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: A is a data source, B are transformation steps, and C is a data sink "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. To run a pipeline you need something called a ________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Your development team is about to execute this code block. What is your team about to do?\n",
    "<img src=\"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/sAk77UylEeiErArshWmU7g_bfa6463a228a9641c785e4dc38904d1e_code1.jpg?expiry=1544054400000&amp;hmac=lEMYzbwyNzZ1fFwititSOrm7rGL78HfM6DNzbT668r4\" alt=\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: We are compiling our Cloud Dataflow pipeline written in Java and are submitting it to the cloud for execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. TRUE or FALSE: A ParDo acts on all items at once (like a Map in MapReduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
